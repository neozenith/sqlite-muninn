<!-- AUTO-GENERATED by benchmark_adjacency_analyze.py — DO NOT EDIT -->
<!-- Regenerate: make -C benchmarks analyze-adjacency -->

# Adjacency Index Benchmarks

Direct comparison of four graph-read strategies after edge mutations.
Each chart plots performance against graph size (node count) on a log-log scale.

## Methods

| Method | Description |
|--------|-------------|
| **TVF** | No cache — scans edge table via SQL on every query |
| **CSR — full rebuild** | Persistent CSR cache; full edge-table re-scan when stale |
| **CSR — incremental** | Delta + merge; rebuilds all blocks (spread mutations) |
| **Blocked CSR — incremental** | Delta + merge; rebuilds only affected blocks (concentrated mutations) |

## How Blocked CSR Works

The CSR is partitioned into blocks of 4,096 nodes. Each block is a separate
row in the shadow table. When edges change, only blocks containing affected
nodes are rewritten — unaffected blocks require zero I/O.

## Degree Query Time

```plotly
--8<-- "benchmarks/charts/adj_degree.json"
```

## Betweenness Query Time

```plotly
--8<-- "benchmarks/charts/adj_betweenness.json"
```

## Closeness Query Time

```plotly
--8<-- "benchmarks/charts/adj_closeness.json"
```

## Leiden Query Time

```plotly
--8<-- "benchmarks/charts/adj_leiden.json"
```

## Rebuild Time

```plotly
--8<-- "benchmarks/charts/adj_rebuild_time.json"
```

## Initial CSR Build Time

```plotly
--8<-- "benchmarks/charts/adj_build_time.json"
```

## Shadow Table Disk Usage

```plotly
--8<-- "benchmarks/charts/adj_disk_usage.json"
```

## Trigger Overhead

```plotly
--8<-- "benchmarks/charts/adj_trigger_overhead.json"
```
